{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering, BertTokenizerFast\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the medical dataset\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "data = data.head(10)\n",
        "# Split the dataset into training and validation sets (80% train, 20% validation)\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    if type(text) != str:\n",
        "      text = str(text)\n",
        "      text = text.lower()\n",
        "\n",
        "    # Remove special characters, numbers, and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to questions and answers columns\n",
        "train_data['questions'] = train_data['question'].apply(preprocess_text)\n",
        "train_data['answers'] = train_data['answer'].apply(preprocess_text)\n",
        "val_data['questions'] = val_data['question'].apply(preprocess_text)\n",
        "val_data['answers'] = val_data['answer'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "Y5Pc5TjlGgYI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and preprocess training data\n",
        "train_inputs = tokenizer(train_data['questions'].tolist(), train_data['answers'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Convert answers to start and end token positions\n",
        "train_start_positions = []\n",
        "train_end_positions = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    encoding = tokenizer(train_data['answers'].iloc[i], return_offsets_mapping=True, padding=True, truncation=True)\n",
        "    start_idx = encoding.char_to_token(0) if encoding.char_to_token(0) is not None else 0\n",
        "    end_idx = encoding.char_to_token(len(train_data['answers'].iloc[i]) - 1) if encoding.char_to_token(len(train_data['answers'].iloc[i]) - 1) is not None else 0\n",
        "    train_start_positions.append(start_idx)\n",
        "    train_end_positions.append(end_idx)\n",
        "\n",
        "# Create TensorDataset for training\n",
        "train_dataset = TensorDataset(train_inputs.input_ids, train_inputs.attention_mask, train_inputs.token_type_ids,\n",
        "                              torch.tensor(train_start_positions), torch.tensor(train_end_positions))\n",
        "\n",
        "# Define DataLoader for training\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da7GCZDIO2bo",
        "outputId": "b7b9df5a-9dc4-4111-c534-553fb90ca1a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT-based QA model\n",
        "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(qa_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "qa_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[0]\n",
        "        attention_mask = batch[1]\n",
        "        token_type_ids = batch[2]\n",
        "        start_positions = batch[3]\n",
        "        end_positions = batch[4]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the trained QA model\n",
        "qa_model.save_pretrained('trained_qa_model')\n",
        "\n",
        "# Tokenize and preprocess validation data\n",
        "val_inputs = tokenizer(val_data['questions'].tolist(), val_data['answers'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Convert answers to start and end token positions\n",
        "val_start_positions = []\n",
        "val_end_positions = []\n",
        "\n",
        "for i in range(len(val_data)):\n",
        "    encoding = tokenizer(val_data['answers'].iloc[i], return_offsets_mapping=True, padding=True, truncation=True)\n",
        "    start_idx = encoding.char_to_token(0)\n",
        "    end_idx = encoding.char_to_token(len(val_data['answers'].iloc[i]) - 1)\n",
        "    val_start_positions.append(start_idx)\n",
        "    val_end_positions.append(end_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j7CF-biPA0w",
        "outputId": "f7647abb-15ad-48a1-8066-f3ad20366c73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorDataset for validation\n",
        "val_dataset = TensorDataset(val_inputs.input_ids, val_inputs.attention_mask, val_inputs.token_type_ids,\n",
        "                            torch.tensor(val_start_positions), torch.tensor(val_end_positions))\n",
        "\n",
        "# Define DataLoader for validation\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the trained QA model on the validation set\n",
        "qa_model.eval()\n",
        "predictions = []\n",
        "\n",
        "for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch[0]\n",
        "        attention_mask = batch[1]\n",
        "        token_type_ids = batch[2]\n",
        "\n",
        "        outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        # Get the best start and end positions\n",
        "        start_index = torch.argmax(start_logits, dim=1)\n",
        "        end_index = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "        # Get predicted answers\n",
        "        for i in range(len(input_ids)):\n",
        "            pred_answer = tokenizer.decode(input_ids[i][start_index[i]:end_index[i]+1])\n",
        "            predictions.append(pred_answer)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(val_data['answers'], predictions, average='micro')\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f38lFTl2PCit",
        "outputId": "4473d9f3-264c-414e-af65-eb7e9cae9541"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(val_data['answers'], predictions, average='micro')\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "Iwb356SOGgVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def mean_reciprocal_rank(targets, predictions):\n",
        "    ranks = []\n",
        "    for target, prediction in zip(targets, predictions):\n",
        "        if target in prediction:\n",
        "            rank = 1 / (prediction.index(target) + 1)\n",
        "            ranks.append(rank)\n",
        "        else:\n",
        "            ranks.append(0)\n",
        "    return np.mean(ranks)\n",
        "\n",
        "def exact_match(targets, predictions):\n",
        "    em = 0\n",
        "    for target, prediction in zip(targets, predictions):\n",
        "        if target == prediction:\n",
        "            em += 1\n",
        "    return em / len(targets)\n",
        "\n",
        "# Evaluate the trained QA model on the validation set\n",
        "qa_model.eval()\n",
        "all_targets = val_data['answers'].tolist()\n",
        "all_predictions = []\n",
        "\n",
        "for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch[0]\n",
        "        attention_mask = batch[1]\n",
        "        token_type_ids = batch[2]\n",
        "\n",
        "        outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        # Get the best start and end positions\n",
        "        start_index = torch.argmax(start_logits, dim=1)\n",
        "        end_index = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "        # Get predicted answers\n",
        "        for i in range(len(input_ids)):\n",
        "            pred_answer = tokenizer.decode(input_ids[i][start_index[i]:end_index[i]+1])\n",
        "            all_predictions.append(pred_answer)\n",
        "\n",
        "# Calculate MRR\n",
        "mrr = mean_reciprocal_rank(all_targets, all_predictions)\n",
        "print(\"Mean Reciprocal Rank (MRR):\", mrr)\n",
        "\n",
        "# Calculate Exact Match (EM)\n",
        "em = exact_match(all_targets, all_predictions)\n",
        "print(\"Exact Match (EM):\", em)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6kZEP5wNSox",
        "outputId": "b6ed0500-c588-4e47-ed58-543f71e266b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reciprocal Rank (MRR): 0.0\n",
            "Exact Match (EM): 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_2l4YmiR50J",
        "outputId": "cdecb589-56c0-46f2-b1b8-d1e93e0b7a47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Encourage comprehensive dilated eye exam least every two year Remember lowering eye pressure glaucoma early stage slows progression disease help save vision Get tip finding eye care professional',\n",
              " 'The optic nerve bundle million nerve fiber It connects retina brain']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekkSV-WPRubZ",
        "outputId": "2295b813-8dff-4e10-ec9d-1b92cd4b9b06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who risk glaucoma', 'what glaucoma']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAb41WViRuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uN7txtG6RuQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}